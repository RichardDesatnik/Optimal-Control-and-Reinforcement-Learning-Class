{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRUhJz057_le"
   },
   "outputs": [],
   "source": [
    "# Feel free to import anything you need\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lt5oHKILqIB3"
   },
   "outputs": [],
   "source": [
    "###########  Do not modify this cell ###########################\n",
    "# ===============================================================\n",
    "\n",
    "### seed everything for reproducibility\n",
    "def seed_everything():\n",
    "    seed = 24789\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI44Q5CfqdNK"
   },
   "source": [
    "# 1. Contrastive learning loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXeDWv33dEww"
   },
   "source": [
    "## 1(a) Implement the normalized temperature-scaled cross entropy loss (NT-Xent) based on SimCLR:\n",
    "https://arxiv.org/pdf/2002.05709.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Io80AHLvn5Wo"
   },
   "source": [
    "\\begin{equation}\n",
    "\\ell_{i, j}=-\\log \\frac{\\exp \\left(\\operatorname{sim}\\left(\\boldsymbol{z}_i, \\boldsymbol{z}_j\\right) / \\tau\\right)}{\\sum_{k=1}^{2 N}  \\mathbb{1}_{[k \\neq i]} \\exp \\left(\\operatorname{sim}\\left(\\boldsymbol{z}_i, \\boldsymbol{z}_k\\right) / \\tau\\right)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax5Ha3k-c2LQ"
   },
   "outputs": [],
   "source": [
    "def nt_xent(x, t=0.5):\n",
    "    \"\"\"\n",
    "    1. Normalize across the second dimension\n",
    "    2. Compute the normalized cosine similarity scores between x and its transpose\n",
    "    3. Scale the normalized cosine similarity scores x_scores by dividing them by a temperature\n",
    "\n",
    "    \"\"\"\n",
    "    # 1. Normalize across the second dimension\n",
    "    x = F.normalize(x, dim=1)\n",
    "\n",
    "    # 2. Compute the normalized cosine similarity scores between x and its transpose and scale the with temperature\n",
    "\n",
    "    x_scores = (x @ x.t()).clamp(min=1e-7)\n",
    "    x_scaled = x_scores / t \n",
    "\n",
    "\n",
    "    # Subtract a diagonal matrix with large negative values from x_scale to set the diagonals to be zeros after softmax. \n",
    "    x_scaled = x_scaled - torch.eye(x_scaled.size(0)).to(x_scaled.device) * 1e5\n",
    "\n",
    "    # targets 2N elements.\n",
    "    targets = torch.arange(x.size()[0])\n",
    "    targets[::2] += 1  # target of 2k element is 2k+1\n",
    "    targets[1::2] -= 1  # target of 2k+1 element is 2k\n",
    "\n",
    "    # 3 . Compute the cross entropy loss between x_scale and target and return the computed loss\n",
    "    loss = F.cross_entropy(x_scaled, targets.long().to(x_scaled.device))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6rhf-WxNoObF",
    "outputId": "19b13e0d-8ad5-4bfe-fe00-072f9bc4a2ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: \n",
      " tensor([[-0.2997, -0.6194, -0.4414],\n",
      "        [-1.2945, -0.1358, -0.2589],\n",
      "        [-0.7526,  0.4345, -0.9680],\n",
      "        [ 0.8034, -2.5620,  0.9044]])\n",
      "Loss: \n",
      " tensor(1.2352)\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "\n",
    "z = torch.randn(4,3)\n",
    "t = 0.7\n",
    "\n",
    "loss = nt_xent(z, t)\n",
    "\n",
    "print(\"z: \\n\", z)\n",
    "print(\"Loss: \\n\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WImNc2GUqY9S"
   },
   "source": [
    "## 1 (b) Implement Barlow twins loss function\n",
    "https://arxiv.org/pdf/2103.03230.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOi6AEu4kD5b"
   },
   "source": [
    "Cross correlation matrix\n",
    "\\begin{equation}\n",
    "\\mathcal{C}_{i j} \\triangleq \\frac{\\sum_b z_{b, i}^A z_{b, j}^B}{\\sqrt{\\sum_b\\left(z_{b, i}^A\\right)^2} \\sqrt{\\sum_b\\left(z_{b, j}^B\\right)^2}}\n",
    "\\end{equation}\\\n",
    "Barlow Twins loss\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{\\mathcal{B} \\mathcal{T}} \\triangleq \\sum_i\\left(1-\\mathcal{C}_{i i}\\right)^2+\\lambda \\quad \\sum_i \\sum_{j \\neq i} \\mathcal{C}_{i j}^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlmHoKI9chQU"
   },
   "outputs": [],
   "source": [
    "def off_diag_elems(x):\n",
    "    \"\"\"\n",
    "    Compute the flattened view of the off-diagonal elements of a matrix\n",
    "\n",
    "    \"\"\"\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n",
    "\n",
    "def barlow_twins(z1, z2, lamda, batch_size):\n",
    "    \"\"\"\n",
    "    z1 and z2 are the output image representations  \n",
    "    1. Normalize the representations across the batch dimension (dim=0)\n",
    "    2. Compute the cross correlation matrix (given in the equation above)\n",
    "    3. Compute the barlow twins loss by following the equations given above\n",
    "\n",
    "    \"\"\"\n",
    "    z1_norm = (z1 - torch.mean(z1, dim=0)) / torch.std(z1, dim=0)\n",
    "    z2_norm = (z2 - torch.mean(z2, dim=0)) / torch.std(z2, dim=0)\n",
    "\n",
    "    C = torch.matmul(z1_norm.T, z2_norm) / batch_size\n",
    "    on_diag = torch.diagonal(C).add_(-1).pow_(2).sum()\n",
    "    off_diag = off_diag_elems(C).pow_(2).sum()\n",
    "\n",
    "    loss = on_diag + lamda * off_diag\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IY7mgqQnjkFi",
    "outputId": "4cfcb066-3efe-44c5-afc4-456be0cfb2b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1: \n",
      " tensor([[-0.6637, -2.2035, -0.5497,  0.1591],\n",
      "        [-0.8427, -0.3029, -1.2701, -0.8752],\n",
      "        [ 0.1743, -0.6272, -0.5871, -1.7928],\n",
      "        [-1.7089,  0.2134, -0.3203, -0.3736]])\n",
      "z2: \n",
      " tensor([[ 0.6968,  1.8388, -0.9387, -0.4800],\n",
      "        [-1.4524,  1.3257,  0.9204, -0.5180],\n",
      "        [-1.0516, -0.5895,  0.5390,  0.0944],\n",
      "        [-1.7974,  0.0804,  0.7257,  0.9933]])\n",
      "Loss: \n",
      " tensor(5.3352)\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "\n",
    "N = 4\n",
    "z1 = torch.randn(N, 4)\n",
    "z2 = torch.randn(N, 4)\n",
    "lamda = 5e-3\n",
    "\n",
    "loss = barlow_twins(z1, z2, lamda, N)\n",
    "\n",
    "print(\"z1: \\n\", z1)\n",
    "print(\"z2: \\n\", z2)\n",
    "print(\"Loss: \\n\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emZ4VWK7nWvr"
   },
   "source": [
    "# 2. Implement the SimCLR model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Txvy1HDAc7P5"
   },
   "outputs": [],
   "source": [
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, base_encoder, projection_dim):\n",
    "        super().__init__()\n",
    "        self.enc = base_encoder(weights=False)  \n",
    "        self.feature_dim = self.enc.fc.in_features\n",
    "\n",
    "        # Modifying the base encoder as mentioned in B4 of SimCLR \n",
    "        self.enc.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
    "        self.enc.maxpool = nn.Identity()\n",
    "        self.enc.fc = nn.Identity()  \n",
    "\n",
    "        # Add MLP projection.\n",
    "        self.projection_dim = projection_dim\n",
    "        self.projector = nn.Sequential(nn.Linear(self.feature_dim, 2048),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(2048, projection_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        ######## Define the forward function ############################\n",
    "        feature = self.enc(x)\n",
    "        projection = self.projector(feature)\n",
    "        return feature, projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpsD3lHyd7Vh"
   },
   "outputs": [],
   "source": [
    "###########  Do not modify this cell ###########################\n",
    "# ===============================================================\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NR1EtdRd8ml"
   },
   "outputs": [],
   "source": [
    "\n",
    "# color distortion composed by color jittering and color dropping.\n",
    "# See Section A of SimCLR: https://arxiv.org/abs/2002.05709\n",
    "def get_color_distortion(s=0.5):  # 0.5 for CIFAR10 by default\n",
    "    # s is the strength of color distortion\n",
    "    color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
    "    rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
    "    rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
    "    color_distort = transforms.Compose([rnd_color_jitter, rnd_gray])\n",
    "    return color_distort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMRlattQyClH"
   },
   "source": [
    "Creating pairs of image on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UUWeeUKgH8Z"
   },
   "outputs": [],
   "source": [
    "###########  Do not modify this cell ###########################\n",
    "# ===============================================================\n",
    "\n",
    "class CIFAR10Pair(CIFAR10):\n",
    "    \"\"\"Generate mini-batche pairs on CIFAR10 training set.\"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx], self.targets[idx]\n",
    "        img = Image.fromarray(img)  # .convert('RGB')\n",
    "        imgs = [self.transform(img), self.transform(img)]\n",
    "        return torch.stack(imgs), target  # stack a positive pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZyRK1GGyQg-"
   },
   "source": [
    "#3. Define the transformations, dataloaders, model, optimizer, scheduler and parameters required for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8TI5Ts910fk"
   },
   "outputs": [],
   "source": [
    "############ Define the parameters for training ##############\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "projection_dim = 128\n",
    "learning_rate =  0.6\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-6\n",
    "epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSKk3wpueZBW",
    "outputId": "a9cb9e4c-f132-4e7e-f9e3-118001383bbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:03<00:00, 42680184.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /content/cifar-10-python.tar.gz to /content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available()\n",
    "cudnn.benchmark = True\n",
    "\n",
    "######### Define the transformation on the training set\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(32),\n",
    "                                        transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                        transforms.ToTensor()])\n",
    "\n",
    "##### get absolute path of data dir\n",
    "\n",
    "data_dir = \"/content\"  \n",
    "\n",
    "\n",
    "########## Define the train set and train dataloader\n",
    "\n",
    "train_set = CIFAR10Pair(root=data_dir,\n",
    "                        train=True,\n",
    "                        transform=train_transform,\n",
    "                        download=True)\n",
    "\n",
    "train_loader = DataLoader(train_set,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=2,\n",
    "                            drop_last=True)\n",
    "\n",
    "\n",
    "# Define the base encoder(resnet18) -- load from torchvision.models without pretraining\n",
    "\n",
    "base_encoder = torchvision.models.resnet18\n",
    "model = SimCLR(base_encoder, projection_dim=projection_dim)\n",
    "model = model.cuda()\n",
    "\n",
    "########### Define the optimizer and scheduler for training #######################\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    learning_rate,\n",
    "    momentum=momentum,\n",
    "    weight_decay=weight_decay,\n",
    "    nesterov=True)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XT-fhx2bc2V6"
   },
   "outputs": [],
   "source": [
    "def train(args) -> None: \n",
    "    # SimCLR training\n",
    "    model.train()\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(1, args['epochs'] + 1):\n",
    "        loss_meter = AverageMeter(\"SimCLR_loss\")\n",
    "        train_bar = tqdm(train_loader)\n",
    "        for x, y in train_bar:\n",
    "            sizes = x.size()\n",
    "            x = x.view(sizes[0] * 2, sizes[2], sizes[3], sizes[4]).cuda(non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ####### Get the output from the model ################\n",
    "\n",
    "            feature, rep = model(x)\n",
    "\n",
    "            ######  Compute the loss using nt-cross entropy loss #####################################\n",
    "\n",
    "            loss = nt_xent(rep, args['temperature'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            loss_meter.update(loss.item(), x.size(0))\n",
    "            train_bar.set_description(\"Train epoch {}, SimCLR loss: {:.4f}\".format(epoch, loss_meter.avg))\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                torch.save(model.state_dict(), 'simclr_best_epoch.pt'.format(epoch))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "zNfD8KQkeCmS",
    "outputId": "6f040845-34d2-4be6-c291-cc14556944a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 1, SimCLR loss: 5.9324: 100%|██████████| 97/97 [01:38<00:00,  1.02s/it]\n",
      "Train epoch 2, SimCLR loss: 5.7052: 100%|██████████| 97/97 [01:14<00:00,  1.30it/s]\n",
      "Train epoch 3, SimCLR loss: 5.6161: 100%|██████████| 97/97 [01:15<00:00,  1.29it/s]\n",
      "Train epoch 4, SimCLR loss: 5.5487: 100%|██████████| 97/97 [01:13<00:00,  1.31it/s]\n",
      "Train epoch 5, SimCLR loss: 5.4926: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 6, SimCLR loss: 5.4549: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 7, SimCLR loss: 5.4258: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 8, SimCLR loss: 5.4081: 100%|██████████| 97/97 [01:13<00:00,  1.31it/s]\n",
      "Train epoch 9, SimCLR loss: 5.3890: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 10, SimCLR loss: 5.3787: 100%|██████████| 97/97 [01:13<00:00,  1.31it/s]\n",
      "Train epoch 11, SimCLR loss: 5.3678: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 12, SimCLR loss: 5.3557: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 13, SimCLR loss: 5.3478: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 14, SimCLR loss: 5.3420: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 15, SimCLR loss: 5.3377: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 16, SimCLR loss: 5.3317: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 17, SimCLR loss: 5.3266: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 18, SimCLR loss: 5.3208: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 19, SimCLR loss: 5.3172: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 20, SimCLR loss: 5.3144: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 21, SimCLR loss: 5.3112: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 22, SimCLR loss: 5.3087: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 23, SimCLR loss: 5.3046: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 24, SimCLR loss: 5.3039: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 25, SimCLR loss: 5.3015: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 26, SimCLR loss: 5.2976: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 27, SimCLR loss: 5.2958: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 28, SimCLR loss: 5.2919: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 29, SimCLR loss: 5.2894: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 30, SimCLR loss: 5.2900: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 31, SimCLR loss: 5.2876: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 32, SimCLR loss: 5.2876: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 33, SimCLR loss: 5.2830: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 34, SimCLR loss: 5.2811: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 35, SimCLR loss: 5.2799: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 36, SimCLR loss: 5.2777: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 37, SimCLR loss: 5.2769: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 38, SimCLR loss: 5.2729: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 39, SimCLR loss: 5.2714: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 40, SimCLR loss: 5.2712: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 41, SimCLR loss: 5.2687: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 42, SimCLR loss: 5.2680: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 43, SimCLR loss: 5.2678: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 44, SimCLR loss: 5.2656: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 45, SimCLR loss: 5.2641: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 46, SimCLR loss: 5.2633: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 47, SimCLR loss: 5.2620: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 48, SimCLR loss: 5.2631: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 49, SimCLR loss: 5.2625: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 50, SimCLR loss: 5.2580: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 51, SimCLR loss: 5.2580: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 52, SimCLR loss: 5.2577: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 53, SimCLR loss: 5.2562: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 54, SimCLR loss: 5.2562: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 55, SimCLR loss: 5.2547: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 56, SimCLR loss: 5.2528: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 57, SimCLR loss: 5.2553: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 58, SimCLR loss: 5.2530: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 59, SimCLR loss: 5.2534: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 60, SimCLR loss: 5.2520: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 61, SimCLR loss: 5.2513: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 62, SimCLR loss: 5.2489: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 63, SimCLR loss: 5.2479: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 64, SimCLR loss: 5.2481: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 65, SimCLR loss: 5.2465: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 66, SimCLR loss: 5.2472: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 67, SimCLR loss: 5.2458: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 68, SimCLR loss: 5.2456: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 69, SimCLR loss: 5.2436: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 70, SimCLR loss: 5.2453: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 71, SimCLR loss: 5.2435: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 72, SimCLR loss: 5.2439: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 73, SimCLR loss: 5.2425: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 74, SimCLR loss: 5.2420: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 75, SimCLR loss: 5.2408: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 76, SimCLR loss: 5.2414: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 77, SimCLR loss: 5.2413: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 78, SimCLR loss: 5.2417: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 79, SimCLR loss: 5.2391: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 80, SimCLR loss: 5.2413: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 81, SimCLR loss: 5.2367: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 82, SimCLR loss: 5.2395: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 83, SimCLR loss: 5.2389: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 84, SimCLR loss: 5.2366: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 85, SimCLR loss: 5.2378: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 86, SimCLR loss: 5.2365: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 87, SimCLR loss: 5.2353: 100%|██████████| 97/97 [01:13<00:00,  1.32it/s]\n",
      "Train epoch 88, SimCLR loss: 5.2363: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 89, SimCLR loss: 5.2351: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 90, SimCLR loss: 5.2387: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 91, SimCLR loss: 5.2348: 100%|██████████| 97/97 [01:13<00:00,  1.33it/s]\n",
      "Train epoch 92, SimCLR loss: 5.2342: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 93, SimCLR loss: 5.2351: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 94, SimCLR loss: 5.2368: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 95, SimCLR loss: 5.2367: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 96, SimCLR loss: 5.2359: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 97, SimCLR loss: 5.2352: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 98, SimCLR loss: 5.2353: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 99, SimCLR loss: 5.2346: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n",
      "Train epoch 100, SimCLR loss: 5.2349: 100%|██████████| 97/97 [01:12<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'epochs':  100,\n",
    "    'batch_size': 512,\n",
    "    'temperature': 0.5,\n",
    "}\n",
    "\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmgKAHPnTz_d"
   },
   "source": [
    "# Finetuning and testing the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9BBAXJwiTzQG"
   },
   "outputs": [],
   "source": [
    "###########  Do not modify this cell ###########################\n",
    "# ===============================================================\n",
    "\n",
    "class LinModel(nn.Module):\n",
    "    \"\"\"Linear wrapper of encoder.\"\"\"\n",
    "    def __init__(self, encoder: nn.Module, feature_dim: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.enc = encoder\n",
    "        self.feature_dim = feature_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.lin = nn.Linear(self.feature_dim, self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(self.enc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "u1lM-GlnTzYa"
   },
   "outputs": [],
   "source": [
    "###########  Do not modify this cell ###########################\n",
    "# ===============================================================\n",
    "\n",
    "def run_epoch(model, dataloader, epoch, optimizer=None, scheduler=None):\n",
    "    if optimizer:\n",
    "        model.train()\n",
    "        print(\"training...........\")\n",
    "    else:\n",
    "        model.eval()\n",
    "        print(\"eval...............\")\n",
    "\n",
    "    loss_meter = AverageMeter('loss')\n",
    "    acc_meter = AverageMeter('acc')\n",
    "    loader_bar = tqdm(dataloader)\n",
    "    for x, y in loader_bar:\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        loss_meter.update(loss.item(), x.size(0))\n",
    "        acc_meter.update(acc.item(), x.size(0))\n",
    "        if optimizer:\n",
    "            loader_bar.set_description(\"Train epoch {}, loss: {:.4f}, acc: {:.4f}\"\n",
    "                                       .format(epoch, loss_meter.avg, acc_meter.avg))\n",
    "        else:\n",
    "            loader_bar.set_description(\"Test epoch {}, loss: {:.4f}, acc: {:.4f}\"\n",
    "                                       .format(epoch, loss_meter.avg, acc_meter.avg))\n",
    "\n",
    "    return loss_meter.avg, acc_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZVZdfHCGZhu1"
   },
   "outputs": [],
   "source": [
    "############################################ Define train and test transforms #########################################################\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(32),\n",
    "                                          transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                          transforms.ToTensor()])\n",
    "test_transform = transforms.ToTensor()\n",
    "\n",
    "data_dir = '/content'\n",
    "\n",
    "###################################### Define train and test dataloader #########################################\n",
    "\n",
    "train_set = CIFAR10(root=data_dir, train=True, transform=train_transform, download=False)\n",
    "test_set = CIFAR10(root=data_dir, train=False, transform=test_transform, download=False)\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=512, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False)\n",
    "\n",
    "################################ Define the model and load the trained weights of the best model ##############################\n",
    "\n",
    "base_encoder = torchvision.models.resnet18\n",
    "pre_model = SimCLR(base_encoder, projection_dim=projection_dim).cuda()\n",
    "pre_model.load_state_dict(torch.load('simclr_best_epoch.pt'))\n",
    "model = LinModel(pre_model.enc, feature_dim=pre_model.feature_dim, n_classes=len(train_set.targets))\n",
    "model = model.cuda()\n",
    "\n",
    "# Fix encoder\n",
    "model.enc.requires_grad = False\n",
    "parameters = [param for param in model.parameters() if param.requires_grad is True]  # trainable parameters.\n",
    "\n",
    "############################# Define the optimizer and scheduler ###################################################\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    parameters,\n",
    "    0.2,   # lr = 0.1 * batch_size / 256, see section B.6 and B.7 of SimCLR paper.\n",
    "    momentum=momentum,\n",
    "    weight_decay=0.,\n",
    "    nesterov=True)\n",
    "\n",
    "# cosine annealing lr\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "g2XRPeRATzbV"
   },
   "outputs": [],
   "source": [
    "def finetune() -> None:\n",
    "   \n",
    "    optimal_loss, optimal_acc = 1e5, 0.\n",
    "    train_losses, test_losses = [], []\n",
    "    train_accuracies, test_accuracies = [], []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = run_epoch(model, train_loader, epoch, optimizer, scheduler)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        test_loss, test_acc = run_epoch(model, test_loader, epoch)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        if train_loss < optimal_loss:\n",
    "            optimal_loss = train_loss\n",
    "            optimal_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'simclr_lin_best.pth')\n",
    "\n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "IbSR0nWqTzeg",
    "outputId": "9e2db404-0655-48f0-cd61-600c36018c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 1, loss: 3.5564, acc: 0.2334: 100%|██████████| 97/97 [01:05<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 1, loss: 1.8549, acc: 0.3163: 100%|██████████| 10000/10000 [01:20<00:00, 124.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 2, loss: 1.8171, acc: 0.3257: 100%|██████████| 97/97 [00:57<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 2, loss: 1.6685, acc: 0.3973: 100%|██████████| 10000/10000 [01:19<00:00, 125.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 3, loss: 1.6869, acc: 0.3794: 100%|██████████| 97/97 [00:57<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 3, loss: 1.5265, acc: 0.4405: 100%|██████████| 10000/10000 [01:20<00:00, 124.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 4, loss: 1.6074, acc: 0.4145: 100%|██████████| 97/97 [00:57<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 4, loss: 1.5196, acc: 0.4548: 100%|██████████| 10000/10000 [01:20<00:00, 124.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 5, loss: 1.5420, acc: 0.4416: 100%|██████████| 97/97 [00:57<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 5, loss: 1.4734, acc: 0.4657: 100%|██████████| 10000/10000 [01:20<00:00, 124.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 6, loss: 1.4648, acc: 0.4694: 100%|██████████| 97/97 [00:58<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 6, loss: 1.3837, acc: 0.5036: 100%|██████████| 10000/10000 [01:20<00:00, 124.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 7, loss: 1.3820, acc: 0.5010: 100%|██████████| 97/97 [00:57<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 7, loss: 1.2399, acc: 0.5540: 100%|██████████| 10000/10000 [01:20<00:00, 124.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 8, loss: 1.3025, acc: 0.5344: 100%|██████████| 97/97 [00:57<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 8, loss: 1.1131, acc: 0.6025: 100%|██████████| 10000/10000 [01:19<00:00, 125.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 9, loss: 1.2306, acc: 0.5607: 100%|██████████| 97/97 [00:57<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 9, loss: 1.1738, acc: 0.5926: 100%|██████████| 10000/10000 [01:21<00:00, 123.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 10, loss: 1.1659, acc: 0.5853: 100%|██████████| 97/97 [00:58<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 10, loss: 1.0552, acc: 0.6269: 100%|██████████| 10000/10000 [01:20<00:00, 124.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 11, loss: 1.1084, acc: 0.6059: 100%|██████████| 97/97 [00:57<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 11, loss: 0.9714, acc: 0.6624: 100%|██████████| 10000/10000 [01:15<00:00, 131.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 12, loss: 1.0573, acc: 0.6258: 100%|██████████| 97/97 [00:57<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 12, loss: 0.9120, acc: 0.6816: 100%|██████████| 10000/10000 [01:12<00:00, 137.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 13, loss: 1.0145, acc: 0.6418: 100%|██████████| 97/97 [00:57<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test epoch 13, loss: 0.9766, acc: 0.6711: 100%|██████████| 10000/10000 [01:12<00:00, 137.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 14, loss: 0.9675, acc: 0.6627:  19%|█▊        | 18/97 [00:10<00:45,  1.72it/s]"
     ]
    }
   ],
   "source": [
    "train_losses, train_accuracies, test_losses, test_accuracies = finetune()\n",
    "plt.plot(train_losses, label='train loss')  # simply visualize the training loss\n",
    "plt.plot(test_losses, label='test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YorIk3SVZatl"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_accuracies, label='train accuracy')  # simply visualize the training loss\n",
    "plt.plot(test_accuracies, label='test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
